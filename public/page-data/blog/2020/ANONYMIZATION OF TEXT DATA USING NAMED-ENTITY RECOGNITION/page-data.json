{"componentChunkName":"component---src-templates-blog-single-js","path":"/blog/2020/ANONYMIZATION OF TEXT DATA USING NAMED-ENTITY RECOGNITION/","result":{"data":{"site":{"siteMetadata":{"title":"Hamza Massaoudi","author":"Hamza Massaoudi"}},"post":{"rawMarkdownBody":"Anonymization of data became a very trendy topic in recent years. It had been widely addressed by the data community due to its growing importance to all companies collecting personal and sensitive information. \n\nWhile structured data is standardized and relatively easy to anonymize, dealing with unstructured data is more tedious. There is no database schema that can be used to measure privacy risk. In this blog, I propose to use a named-entity recognition system (NER) to automatically detect textual confidential attributes such as identifiers, sensitive information, etc. In this case study, I will consider that these confidential information to be detected are disease names in medical diagnoses.\n\n# Named-Entity Recognition\n\nNamed-entity recognition (also known as entity identification) seeks to identify and classify words in an unstructured text into pre-defined categories.\n\nNER is a very challenging learning problem. On the one hand, Supervised training data is very scarce. On the other, this task requires language specific knowledge to construct efficient structured features.\n\n In our example, we are looking to anonymize medical diagnoses reports by identifying disease names. Thus, our problem is equivalent to a binary classification of names.\n\n> Example : **Testicular cancer ** and **endometriosis**  have increased in incidence during the last decades .\n\n\n\n# Methodology\n\n![method](/images/uploads/blog2020/methodo.png)\n\nAs proof of concept, We will be focusing on locating disease names in medical reports using a model based on conditional random fields. \n\nIn practice, given a sentence, the model will tag each word with a `\"DISEASE\" ` tag if it is a disease name and ` \"O\" ` tag otherwise,  which indicates that a token belongs to no chunk (outside).\n\n1. First, we load the labeled data, which is a list of sentences and their corresponding labels\n2. The second step is the tokenizer, which splits sentences into tokens\n3. We use a feature generator to extract reliable features with a window of 3 words (the current word, the previous and the next words)\n4. The final step uses a CRFs to train a NER model.\n\n# Training Data\n\nIn our experiments, we took advantage of medical texts that were labeled to study the semantic relationships between diseases and treatments. These [files](https://biotext.berkeley.edu/dis_treat_data.html) were obtained from MEDLINE 2001 using the first 100 titles and the first 40 abstracts from the 59 files medline01n*.xml. These data contain 3,654 labeled sentences. The labels are: ”DISONLY”, ”TREATONLY”, ”TREAT PREV”, ”DIS PREV”, ”TREAT SIDE EFF”, ”DIS SIDE EFF”, ”DIS VAG”, ”TREAT VAG”, ”TREAT NO” and ”DIS NO”. Because we were only interested in diseases, we only used the 629 sentences with the ”DISONLY” label.\n\nAfter formatting and tokenizing raw text data, it looks like this :\n\n```python\nimport random\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nnlp = English()\n# Create a blank Tokenizer with just the English vocab\ntokenizer = Tokenizer(nlp.vocab)\nfilepath = 'data/sentences_with_roles_and_relations.txt'\ndisease_data = read_text_labeled_sentences(filepath, tokenizer)\nsample_data_point = random.choice(disease_data)\nprint(\"Tokens:\\n {}\".format(sample_data_point[0]))\nprint(\"Labels:\\n {}\".format(sample_data_point[1]))\n\n```\n\nOut:\n\n![method](/images/uploads/blog2020/input_data.png)\n\nSo basically, disease data is a list of tuples, each tuple (tokens, labels) represent a sentence divided into tokens and corresponding labels. Where 1 stands for disease name and 0 for other than disease names.\n\nThe disease name in the example above  is \"head-neck carcinomas\". Thus, the last 2 labels are both equal to 1.\n\n# Conditional Random Fields\n\nCRFs have seen wide application in many areas, including natural language processing and computer vision. They are often used for structured prediction and tasks that require predicting variables that depend on each other as well as on observed variables.  \n\nCRFs models combine the ability of graphical models to compactly model the dependence between multivariate data and the ability of classification methods to predict outputs using large sets of input features. \n\nThese models are considered to be the discriminative equivalents of the hidden Markov models. But first, let's explain what is the difference between generative and discriminative models.\n\n#### Generative and Discriminative Models\n\nGenerative models' approach might seem counterintuitive. They describe how a target vector y (type of words in our case) can probabilistically \"generate\" a feature vector x (words in our case). Discriminative models are more intuitive because they are working backward, they describe how to assign a label y to a feature vector x.\n\nIn principle, we can see that the approaches are distinct. They work in two opposite directions,  but theoretically, we can always convert between the two methods using Bayes rule.\n\nFor example, in the naive Bayes model, it is easy to convert the joint probability **p(x,y) = p(y)p(x/y)** into a conditional distribution **p(y/x)**. But in practice, we never have the exact true distribution to calculate the conditional distribution. We can end up with two different estimations of p(y/x).\n\nTo sum up, Generative and discriminative may have the same purpose which is calculating the conditional probability p(y/x), but they proceed in two different ways.\n\n> The difference between generative models and CRFs is exactly analogous to the difference between the naive Bayes and logistic regression classifiers.\n\nCRFs methods can be seen as the discriminative analog of  generative Hidden Markov models. They can also be understood as a generalization of the logistic regression classifier to arbitrary graphical structures.\n\nSince our named-entity recognition task relies on predicting labels based on context and not only on each word's features, CRFs methods might be a good choice to begin with.\n\nWe will try in the next section ton implement CRFs methods using [pycrfsuite](https://python-crfsuite.readthedocs.io/en/latest/).\n\n#### Implementation\n\nNow that we explained the motivation behind using CRFs model for Named Entity recognition, let's dive directly into code.\n\nFirst, we begin by calculating features for each word with a window of 3 words, which means that we also include features of next and previous words.\n\nHere, we calculate features like word parts, POS tags, word dependencies, lemma, shape (the shape of the word, example: \"CRFs\" -> \"XXXx\"), and other boolean variables like isupper (check if characters are in uppercase), istitle (check if the first character is in uppercase), isdigit (check whether the word consists of digits only), is_stop (check whether the word is a stop word), etc.\n\nSklearn-crfsuite supports several input formats; here we use feature lists.\n\nThe code below was adapted from the [official documentation](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html)\n\n\n\n```python\nimport random\nimport pycrfsuite\ndef word2features(train_sample, i):\n    token = train_sample[i]\n    word = token.text\n    features = [\n        'bias',\n        'word.lower=' + word.lower(),\n        'word[-3:]=' + word[-3:],\n        'word[-2:]=' + word[-2:],\n        'word.isupper=%s' % word.isupper(),\n        'word.istitle=%s' % word.istitle(),\n        'word.isdigit=%s' % word.isdigit(),\n        'word.pos='+token.pos_,\n        'word.dep='+token.dep_,\n        'word.is_stop=%s' %token.is_stop,\n        'word.lemma=' + token.lemma_,\n        'word.tag=' + token.tag_,\n        'word.shape=' + token.shape_,\n        'word.is_alpha=%s' %token.is_alpha,        \n    ]\n    if i > 0:\n        token1 = train_sample[i-1]\n        word1 = token1.text\n        features.extend([\n            '-1:word.lower=' + word1.lower(),\n            '-1:word.istitle=%s' % word1.istitle(),\n            '-1:word.isupper=%s' % word1.isupper(),\n            '-1:word.pos='+token1.pos_,\n            '-1:word.dep='+token1.dep_,\n            '-1:word.is_stop=%s' %token1.is_stop,\n            '-1:word.lemma=' + token1.lemma_,\n            '-1:word.tag=' + token1.tag_,\n            '-1:word.shape=' + token1.shape_,\n            '-1:word.is_alpha=%s' %token1.is_alpha,    \n        ])\n    else:\n        features.append('BOS')\n        \n    if i < len(train_sample)-1:\n        token1 = train_sample[i+1]\n        word1 = token1.text\n        features.extend([\n            '+1:word.lower=' + word1.lower(),\n            '+1:word.istitle=%s' % word1.istitle(),\n            '+1:word.isupper=%s' % word1.isupper(),\n            '+1:word.pos='+token1.pos_,\n            '+1:word.dep='+token1.dep_,\n            '+1:word.is_stop=%s' %token1.is_stop,\n            '+1:word.lemma=' + token1.lemma_,\n            '+1:word.tag=' + token1.tag_,\n            '+1:word.shape=' + token1.shape_,\n            '+1:word.is_alpha=%s' %token1.is_alpha,   \n        ])\n    else:\n        features.append('EOS')       \n    return features\n\ndef sent2features(train_sample):\n    return [word2features(train_sample, i) for i in range(len(train_sample))]\n\ndef encode_labels(labels):\n    return [\"DISEASE\" if label==1 else \"O\" for label in labels]\n```\n\nWe use the encode_labels function to encode integer labels (0 or 1) to string labels (\"DISEASE\" or \"O\") to respect the target format needed by Sklearn-crfsuite.\n\n```python\nrandom.shuffle(disease_data)\ntraining_data = disease_data[int(0.3*len(disease_data)):]\ntest_data = disease_data[:int(0.3*len(disease_data))]\n#%%\nX_train = [sent2features(s[0]) for s in training_data]\ny_train = [encode_labels(s[1]) for s in training_data]\n\nX_test = [sent2features(s[0]) for s in test_data]\ny_test = [s[1] for s in test_data]\n```\n\nOnce we defined the training and the test sets, we can begin training the model.\n\n```Python\ntrainer = pycrfsuite.Trainer(verbose=False)\n\nfor xseq, yseq in zip(X_train, y_train):\n    trainer.append(xseq, yseq)\n\ntrainer.set_params({\n    'c1': 0.44,   # coefficient for L1 penalty\n    'c2': 1e-4,  # coefficient for L2 penalty\n    'max_iterations': 60,  # stop earlier\n\n    # include transitions that are possible, but not observed\n    'feature.possible_transitions': True\n})\nprint(\"Model's parameters : {}\".format(trainer.params()))\ntrainer.train('models/crf_model.crfsuite')\nprint(\"Last iteration log {}\".format(trainer.logparser.last_iteration))\n```\n\nOut:\n\n![log](/images/uploads/blog2020/CRF_log.png)\n\nWe begin by defining a trainer which is mainly our CRFs model, we then define some parameters like c1 and c2 (Coefficients used for regularization) and the max_iteration parameter (used to stop model's iterations). After training, the model is automatically stored in the directory specified in the train function.\n\n# Results and conclusion \n\nAfter training the model, let's see how to calculate the labels for a given test example:\n\n```Python\ntagger = pycrfsuite.Tagger()\ntagger.open(\"models/crf_model.crfsuite\")\n\nprint(\"sentence: {}\".format(test_data[0][0]))\nprint(\"predicted labels: {}\". format(tagger.tag(X_test[0])))\nprint(\"real labels {}\".format(encode_labels(y_test[0])))\n\n```\n\nOut:\n\n![output](/images/uploads/blog2020/output.png)\n\nAs we can see, the model predicts it right this time, but it may surely miss some disease names. Let's measure the overall performance.\n\n```python\noutputs = []\nfor i in range(len(X_test)):\n    outputs.append(tagger.tag(X_test[i]))\n\ntargets = sum(y_test, [])\noutputs = sum(outputs, [])\noutputs = [0 if output==\"O\" else 1 for output in outputs]\n\nprint(\"conf_matrix: \\n\", confusion_matrix(targets, outputs))\nprint(\"precision score:\\n\", precision_score(targets, outputs))\nprint(\"recall score:\\n\", recall_score(targets, outputs))\nprint(\"F1 score:\\n\", f1_score(targets, outputs))\n```\n\nOut:\n\n![measure](/images/uploads/blog2020/measure.png)\n\nWe can see that the precision is much better than recall, which means that the number of false positives (Other names that are detected as disease names) is not that high. Still, there is a lot of disease names that are undetectable by the model. It might be explained by the size of the training dataset.\n\nIt is worth mentioning that this model is very sensitive to features choice. I recommend the reader to delete or add features of his choice to test the model.\n\nCRFs are indeed well suited to named-entity recognition task, but other deep learning models have shown a better performance. For example, RNNs architectures are known to be able to capture the dependence between input variables. Some recent works also include contextual embedding of words using attention (BERT, XLNet, etc.) which might boost model's performance.\n\nSo, that’s it for today, I hope it helped you understand how to apply NER task to text annonymization and how to implement your CRFs model. Feel free to share the blog if you want to.\n\nYou can find full code in my [GitHub page](https://github.com/hamzamassaoudi/CRF_NER)\n\n# Reference\n\n1. [*Classifying Semantic Relations in Bioscience Text*, Barbara Rosario and Marti A. Hearst, in the proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), Barcelona, July 2004.](https://biotext.berkeley.edu/dis_treat_data.html)\n2. [sklearn-crfsuite documentation](https://sklearn-crfsuite.readthedocs.io/en/latest/index.html)\n3. [An Introduction to Conditional Random Fields By Charles Sutton and Andrew McCallum](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)\n4. Anonymization of Unstructured Data Via Named-Entity Recognition by Fadi Hassan Et Al.\n\n\n\n\n\n\n\n\n\n ","fields":{"slug":"/blog/2020/ANONYMIZATION OF TEXT DATA USING NAMED-ENTITY RECOGNITION/","editLink":"https://github.com/hamzamassaoudi/hmassaoudi.com/edit/master/src/pages/blog/2020/ANONYMIZATION OF TEXT DATA USING NAMED-ENTITY RECOGNITION.md"},"frontmatter":{"title":"Anonymization of text data using Named-Entity Recognition","date":"January 08, 2020","excerpt":"Using Conditional Random Fields to anonymize sensitive information","cardimage":{"publicURL":"/static/serverless-r-card-f220399b0ac4f032247e665ab77912d5.jpg"}}}},"pageContext":{"slug":"/blog/2020/ANONYMIZATION OF TEXT DATA USING NAMED-ENTITY RECOGNITION/","previous":null,"next":null}}}